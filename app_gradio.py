import os
import gradio as gr
from dotenv import load_dotenv

# --- RAG í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.messages import HumanMessage, AIMessage

# --- 1. RAG ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™” ---

print("RAG ì±—ë´‡ ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™” ì¤‘...")

# 1-1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("OPENAI_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# 1-2. ëª¨ë¸ ì´ˆê¸°í™” (LLM, Embeddings)
# (GradioëŠ” ì•± ì‹¤í–‰ ë‚´ë‚´ ì‚´ì•„ìˆìœ¼ë¯€ë¡œ ì „ì—­ ë³€ìˆ˜ë¡œ ë¡œë“œ)
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1, streaming=True)
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# 1-3. FAISS ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ
VECTORSTORE_PATH = "faiss_index"
if not os.path.exists(VECTORSTORE_PATH):
    raise FileNotFoundError(f"'{VECTORSTORE_PATH}' í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

vectorstore = FAISS.load_local(
    VECTORSTORE_PATH, 
    embeddings,
    allow_dangerous_deserialization=True
)

# 1-4. ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±
retriever = vectorstore.as_retriever(search_kwargs={'k': 3})

print("LLM, Retriever(FAISS) ë¡œë“œ ì™„ë£Œ.")

# --- 2. (ì‹ ê·œ) RAG ëª¨ë“œë³„ ì²´ì¸ ìƒì„± í•¨ìˆ˜ ---

# ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í”„ë¡¬í”„íŠ¸ìš© ë¬¸ìì—´ë¡œ í¬ë§·íŒ…
def format_docs_with_sources(docs):
    formatted_list = []
    for doc in docs:
        source_filename = os.path.basename(doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ'))
        formatted_entry = (
            f"Source: {source_filename}\n"
            f"Content: {doc.page_content}"
        )
        formatted_list.append(formatted_entry)
    return "\n\n---\n\n".join(formatted_list)

# 2-1. [RAG On] RAG + ëŒ€í™” ê¸°ë¡ ì²´ì¸ ìƒì„± (í”„ë¡¬í”„íŠ¸ ìˆ˜ì •ë¨)
def create_chain_with_kb(retriever, llm):
    """
    RAG(ê²€ìƒ‰)ì™€ ëŒ€í™” ê¸°ë¡ì„ ëª¨ë‘ ì‚¬ìš©í•˜ëŠ” ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    [ìˆ˜ì •ë¨] RAG ë¬¸ë§¥ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ ì—†ìœ¼ë©´, ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•˜ë„ë¡ í”„ë¡¬í”„íŠ¸ë¥¼ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.
    """
    
    # [ìˆ˜ì •] RAGìš© í”„ë¡¬í”„íŠ¸: AIê°€ ìƒí™©ì— ë§ê²Œ RAG/ì¼ë°˜ ë‹µë³€ì„ ì„ íƒí•˜ë„ë¡ ì§€ì‹œ
    rag_prompt = ChatPromptTemplate.from_messages([
        ("system", """
ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì•„ë˜ [ë¬¸ë§¥]ì€ ì‚¬ìš©ìì˜ ê³¼ì œë¬¼(ë¬¸ì„œ)ì—ì„œ ê²€ìƒ‰ëœ ë‚´ìš©ì…ë‹ˆë‹¤.

1.  ë¨¼ì € [ì§ˆë¬¸]ì´ [ë¬¸ë§¥]ì˜ ë‚´ìš©ê³¼ ê´€ë ¨ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
2.  **ë§Œì•½ [ë¬¸ë§¥]ê³¼ ê´€ë ¨ì´ ìˆë‹¤ë©´:** [ë¬¸ë§¥]ì„ ë°”íƒ•ìœ¼ë¡œ [ì§ˆë¬¸]ì— ë‹µë³€í•´ ì£¼ì„¸ìš”. ê·¸ë¦¬ê³  ë‹µë³€ ë§ˆì§€ë§‰ì— (ì¶œì²˜: [Source íŒŒì¼ëª…])ì„ ê¼­ ëª…ì‹œí•˜ì„¸ìš”.
3.  **ë§Œì•½ [ë¬¸ë§¥]ê³¼ ê´€ë ¨ì´ ì—†ê±°ë‚˜** [ì§ˆë¬¸]ì´ ì¼ë°˜ì ì¸ ëŒ€í™”(ì¸ì‚¬, ë†ë‹´, ë‚ ì”¨ ë“±)ë¼ë©´: [ë¬¸ë§¥]ì„ ë¬´ì‹œí•˜ê³ , ë‹¹ì‹ ì˜ ì¼ë°˜ ì§€ì‹ì„ ì‚¬ìš©í•´ ë‹µë³€í•˜ì„¸ìš”. ì´ë•ŒëŠ” ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì§€ ë§ˆì„¸ìš”.

---
[ë¬¸ë§¥]
{context}
"""),
        MessagesPlaceholder(variable_name="chat_history"), # ëŒ€í™” ê¸°ë¡
        ("human", "{input}"), # í˜„ì¬ ì§ˆë¬¸
    ])
    
    # ë¬¸ë§¥(Context) ê²€ìƒ‰ ì²´ì¸ (ê¸°ì¡´ê³¼ ë™ì¼)
    context_chain = (
        (lambda x: x["input"]) 
        | retriever 
        | format_docs_with_sources
    )
    
    # RAG ì²´ì¸ êµ¬ì„± (ê¸°ì¡´ê³¼ ë™ì¼)
    rag_chain = (
        {
            "context": context_chain,
            "chat_history": lambda x: x["chat_history"],
            "input": lambda x: x["input"]
        }
        | rag_prompt
        | llm
        | StrOutputParser()
    )
    return rag_chain

# 2-2. [RAG Off] ì¼ë°˜ ëŒ€í™” + ëŒ€í™” ê¸°ë¡ ì²´ì¸ ìƒì„±
def create_chain_without_kb(llm):
    """RAG(ê²€ìƒ‰) ì—†ì´, ëŒ€í™” ê¸°ë¡ë§Œ ì‚¬ìš©í•˜ëŠ” ì¼ë°˜ ì±„íŒ… ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    
    # ì¼ë°˜ ëŒ€í™”ìš© í”„ë¡¬í”„íŠ¸
    chat_prompt = ChatPromptTemplate.from_messages([
        ("system", "ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}"),
    ])
    
    # ì¼ë°˜ ì±„íŒ… ì²´ì¸
    chat_chain = (
        chat_prompt
        | llm
        | StrOutputParser()
    )
    return chat_chain

# --- 3. (ë³€ê²½) Gradio ì±„íŒ… ì‘ë‹µ í•¨ìˆ˜ ---
# (GradioëŠ” ë¦¬ì†ŒìŠ¤ë¥¼ ì „ì—­(Global)ì— ë‘ê³ , í•¨ìˆ˜ëŠ” í˜¸ì¶œë  ë•Œë§ˆë‹¤ ì‹¤í–‰)

# ì²´ì¸ì„ ë¯¸ë¦¬ ìƒì„± (RAG On/Off ëª¨ë‘)
rag_chain_with_kb = create_chain_with_kb(retriever, llm)
rag_chain_without_kb = create_chain_without_kb(llm)

def chat_response_generator(message, history, use_kb):
    """
    Gradio ChatInterfaceê°€ í˜¸ì¶œí•  ë©”ì¸ í•¨ìˆ˜.
    - message: ì‚¬ìš©ìì˜ í˜„ì¬ ì…ë ¥ (ë¬¸ìì—´)
    - history: ì´ì „ ëŒ€í™” ê¸°ë¡ (Gradio í˜•ì‹: [[user, ai], [user, ai], ...])
    - use_kb: 'additional_inputs'ë¡œ ì¶”ê°€í•œ ì²´í¬ë°•ìŠ¤ì˜ ê°’ (True/False)
    """
    
    # 1. (ì‹ ê·œ) Gradioì˜ 'history'ë¥¼ LangChainì˜ 'Message' í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    langchain_history = []
    for user_msg, ai_msg in history:
        langchain_history.append(HumanMessage(content=user_msg))
        langchain_history.append(AIMessage(content=ai_msg))

    # 2. (ì‹ ê·œ) RAG ëª¨ë“œ(use_kb)ì— ë”°ë¼ ì‚¬ìš©í•  ì²´ì¸ ì„ íƒ
    if use_kb:
        chain = rag_chain_with_kb
    else:
        chain = rag_chain_without_kb

    # 3. (ì‹ ê·œ) ì²´ì¸ì— ì…ë ¥í•  ë”•ì…”ë„ˆë¦¬ ìƒì„±
    input_dict = {
        "input": message,
        "chat_history": langchain_history
    }
    
    # 4. (ê¸°ì¡´ ë¡œì§) ì²´ì¸ ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
    response_stream = chain.stream(input_dict)
    
    partial_response = ""
    for chunk in response_stream:
        partial_response += chunk
        yield partial_response # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‘ë‹µ ë°˜í™˜

# --- 4. (ë³€ê²½) Gradio ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰ ---

# gr.ChatInterface: ì±„íŒ… UIë¥¼ ì¦‰ì‹œ ìƒì„±
# [í•µì‹¬] 'additional_inputs'ì— ì²´í¬ë°•ìŠ¤ë¥¼ ì¶”ê°€
gr.ChatInterface(
    fn=chat_response_generator, # ì‘ë‹µì„ ìƒì„±í•  í•¨ìˆ˜
    
    # [ì‹ ê·œ] RAG ëª¨ë“œ On/Off ì²´í¬ë°•ìŠ¤ ì¶”ê°€
    additional_inputs=[
        gr.Checkbox(
            label="ê³¼ì œë¬¼(RAG) ê²€ìƒ‰ ì‚¬ìš©", 
            value=True, # ê¸°ë³¸ê°’ True
            info="ì²´í¬ ì‹œ: ê³¼ì œë¬¼ ë‚´ìš©ì„ ê²€ìƒ‰í•˜ì—¬ ë‹µë³€í•©ë‹ˆë‹¤.\nì²´í¬ í•´ì œ ì‹œ: ì¼ë°˜ AI ì±—ë´‡ì²˜ëŸ¼ ëŒ€í™”í•©ë‹ˆë‹¤."
        )
    ],
    
    title="ğŸ¤– ë‚˜ì˜ ê³¼ì œë¬¼ RAG ì±—ë´‡ (V2)",
    description="LangChainê³¼ Gradioë¡œ êµ¬ì¶•í•œ RAG ì±—ë´‡ (RAG On/Off, ëŒ€í™”ê¸°ë¡ ê¸°ëŠ¥ íƒ‘ì¬)",
    
    # # [ìˆ˜ì •] "ë¦¬ìŠ¤íŠ¸ì˜ ë¦¬ìŠ¤íŠ¸" í˜•ì‹ìœ¼ë¡œ ë³€ê²½
    # # ê° ì˜ˆì‹œê°€ [ì§ˆë¬¸(message), RAGì²´í¬ë°•ìŠ¤(use_kb)]ì˜ ì§ì„ ì´ë£¸
    # examples=[
    #     ["[ì—¬ê¸°ì— ì˜ˆì‹œ ì§ˆë¬¸ 1 ì…ë ¥]", True], # ì˜ˆì‹œ1 (RAG ì¼  ìƒíƒœ)
    #     ["[ì—¬ê¸°ì— ì˜ˆì‹œ ì§ˆë¬¸ 2 ì…ë ¥]", False]  # ì˜ˆì‹œ2 (RAG ëˆ ìƒíƒœ)
    # ],
    
    # cache_examples=False 
).launch(share=True)